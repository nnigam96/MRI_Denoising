{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_project_demo.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECE 697 - Final Project Demo Notebook\n",
        "### By: Cameron Craig, Justin Hiemstra\n",
        "---\n",
        "Thank you for evaluating our work on the ECE 697 Capstone Project. The purpose of this notebook is to provide a high-level interface from which to run the main components of our code. This is intended to facilitate the reproduction of our results, and to allow interested parties to perform forward passes through our trained model to enhance their own medical image data.\n",
        "\n",
        "This notebook covers the primary accomplishments of the project, but it does not cover everything in the codebase. There are other python notebooks which contain our work exploring and analyzing data, and the generation of synthetic data. Additionally, code and shell scripts related to our work using CycleGAN to correct noise and imhomogeneity are available as a submodule of this repository.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "j00FyiYyOQno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Clone the repository and unzip sample data"
      ],
      "metadata": {
        "id": "wUl5YkKAQvv7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_TtOR1uNaVT"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ccraig3/ece697-mri-denoising.git\n",
        "!mv ece697-mri-denoising/* ./\n",
        "!rm -rf ece697-mri-denoising\n",
        "!unzip sample_brains.zip\n",
        "!unzip sample_knees.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install needed packages"
      ],
      "metadata": {
        "id": "QVTBRvwMU4Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install piq\n",
        "!pip install antspyx\n",
        "!pip install nibabel\n",
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "id": "8RAesn4hU3M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "uSwaYe9CVFJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import cm\n",
        "import ants\n",
        "import cv2\n",
        "from lightning_unet import LitUNet\n",
        "from mri_sup_dataset import MriSupDataset\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms, _utils\n",
        "import os\n",
        "import pytorch_lightning as pl\n",
        "import pickle as pkl\n",
        "from unet import UNet\n",
        "import piq\n",
        "import nibabel as nib\n",
        "import math"
      ],
      "metadata": {
        "id": "jXZbXgJEVGbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### 2. Train a sample version of the model using the provided sample resources\n",
        "\n",
        "The weights will be saved under a new directory called 'model_ckpts'"
      ],
      "metadata": {
        "id": "o6zXL6UfOcAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You need to have a gpu to run this line\n",
        "!python3 train_unet.py --train_imgs 'sample_knees' --val_imgs 'sample_brains' --test_imgs 'sample_brains' --train_bias '25_sample_synth_bias_fields.pkl' --val_bias '25_sample_synth_bias_fields.pkl' --test_bias '25_sample_synth_bias_fields.pkl' --proj_name 'UNet-L1+L2-Demo' --run_name 'my first run' --max_epochs 10 --batch_size 25 --wf 6"
      ],
      "metadata": {
        "id": "UzaBmlXSTjNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### 3. Evaluate your trained model on the sample test set"
      ],
      "metadata": {
        "id": "VWqqdu9mTjna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CKPT_PATH = r'model_ckpts/epoch=9.ckpt'\n",
        "\n",
        "def my_criterion(prediction, y):\n",
        "  return F.mse_loss(prediction, y) + F.l1_loss(prediction, y)\n",
        "\n",
        "unet_model = LitUNet.load_from_checkpoint(CKPT_PATH)\n",
        "unet_model.eval()"
      ],
      "metadata": {
        "id": "1jN6acPSTuAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed Model Parameters\n",
        "SIZE = (320, 320) # (Height, Width) of the generated bias field\n",
        "num_points = 60 # resolution of the simulated coil array\n",
        "\n",
        "# Uniform Random Variable Bounds\n",
        "LOW_BOOST_BOUNDS = (0, 0.2)\n",
        "COIL_VERT_POS_BOUNDS = (SIZE[0] - 1, SIZE[0] * 1.2)\n",
        "PARAM_B_ADJUST_BOUNDS = (-0.02, 0.02)\n",
        "COIL_WIDTH_BOUNDS = (0.1, 0.4)\n",
        "\n",
        "# Helper functions\n",
        "\n",
        "def poly_dec(x):\n",
        "  return 1927.5 * (x + 37)**-2.093\n",
        "\n",
        "# Normalize pixel values to the range [0, 1]\n",
        "def normalize(image):\n",
        "  new_image = image - np.min(image)\n",
        "  return new_image / np.max(new_image)\n",
        "\n",
        "# Obtain a sample of a uniform random variable on the specified bounds\n",
        "def sampleRV(BOUNDS):\n",
        "  return np.random.uniform(BOUNDS[0], BOUNDS[1])\n",
        "\n",
        "def normalize(image):\n",
        "  new_image = image - np.min(image)\n",
        "  return new_image / np.max(new_image)\n",
        "\n",
        "def clip_img(image):\n",
        "  image = np.where(image > 1, 1, image)\n",
        "  return np.where(image < 0, 0, image)\n",
        "\n",
        "def add_rician_noise(image, intensity=1):\n",
        "  n1 = np.random.normal(0, 1, image.shape)\n",
        "  n1 = n1 / np.max(n1)\n",
        "  n2 = np.random.normal(0, 1, image.shape)\n",
        "  n2 = n2 / np.max(n2)\n",
        "  return clip_img(np.abs(image + intensity*n1 + intensity*n2*1j))\n",
        "\n",
        "#Create a Bias Field\n",
        "def genBiasField(SIZE, coil_left, coil_right, coil_vert_pos, b_adj, low_boost):\n",
        "  global a, b, c, d\n",
        "\n",
        "  # Define Coil Shape\n",
        "  cx = np.linspace(round(SIZE[1]*coil_left), round(SIZE[1]*coil_right), num=num_points) # Horizontal coordinates\n",
        "\n",
        "  # Put coil array at or below bottom edge of bias field\n",
        "  y_pos = round(coil_vert_pos)\n",
        "  cy = np.linspace(y_pos, y_pos, num=num_points) # Vertical coordinates\n",
        "\n",
        "  coils = np.stack([cy, cx], axis=0).T # Reshape to prepare for arithmetic operations\n",
        "\n",
        "  B = np.zeros(SIZE)\n",
        "  dists = np.zeros((coils.shape[0],)) # Distances between coil points and field points\n",
        "\n",
        "  # Exponential curve random perturbations\n",
        "  #local_b = b + b_adj\n",
        "\n",
        "  # Loop over all pixels in B\n",
        "  for i in range(B.shape[0]):\n",
        "    for j in range(B.shape[1]):\n",
        "      # Stack of copies of this point's coordinates\n",
        "      p = np.array([i, j])\n",
        "      p = np.tile(p, (num_points, 1))\n",
        "\n",
        "      # Get the distance between this point and the closest coil point\n",
        "      dist = np.min(np.linalg.norm(coils - p, axis=1))\n",
        "\n",
        "      # Simulate exponential falloff\n",
        "      #B[i, j] = exp_dec(dist, a, local_b, c, d)\n",
        "      B[i, j] = poly_dec(dist)\n",
        "  \n",
        "  # Normalize B on range [0, 1]\n",
        "  B_norm = normalize(B)\n",
        "\n",
        "  # Scale up / boost the weak end intensity of the field\n",
        "  B_boosted = B_norm * (1 - low_boost) + low_boost\n",
        "\n",
        "  return B_boosted\n",
        "\n",
        "def genCompositeField(SIZE, lb):\n",
        "  num_coils = 1 #random.randint(1, 3)\n",
        "\n",
        "  coil_width = 0.3 #sampleRV(COIL_WIDTH_BOUNDS)\n",
        "  if num_coils == 1:\n",
        "    coil_width += 0.1\n",
        "  coil_left_bound, coil_right_bound = 0.5 - coil_width, 0.5 + coil_width # horizontal extent of coil array\n",
        "  sub_fields = np.zeros((num_coils, SIZE[0], SIZE[1]))\n",
        "  c_fraction = 1. / num_coils\n",
        "  vert_pos = 350 #sampleRV(COIL_VERT_POS_BOUNDS)\n",
        "  b_adj = 0 #sampleRV(PARAM_B_ADJUST_BOUNDS)\n",
        "  low_boost = lb #sampleRV(LOW_BOOST_BOUNDS)\n",
        "\n",
        "  for i in range(num_coils):\n",
        "    sub_fields[i, :, :] = genBiasField(SIZE, coil_left_bound*c_fraction + i*c_fraction, coil_right_bound*c_fraction + i*c_fraction, vert_pos, b_adj, low_boost)\n",
        "\n",
        "  return sub_fields.mean(axis=0)\n",
        "\n",
        "def run_n4(img, mask):\n",
        "  img = img.copy()\n",
        "  mask = mask.copy()\n",
        "  ants_img = ants.from_numpy((img * 255.).astype('uint8').T)\n",
        "  ants_mask = ants.from_numpy(mask.T)\n",
        "  ants_mask = ants_mask / ants_mask.max()\n",
        "  ants_mask = ants_mask.threshold_image( 1, 2 )\n",
        "  n4_corr = ants.n4_bias_field_correction(ants_img, mask=ants_mask, rescale_intensities=True)\n",
        "  #n4_corr = ants.abp_n4(ants_img)\n",
        "  return n4_corr.numpy().T / 255.\n",
        "\n",
        "def predict(x):\n",
        "  x_np = x.reshape((320, 320)) * 255.\n",
        "  x_np = x_np.astype('uint8')\n",
        "  x_tensor = transform(x_np).float()\n",
        "  x_tensor -= x_tensor.min()\n",
        "  x_tensor /= (x_tensor.max() + 1e-9)\n",
        "  y_hat = unet_model(x_tensor.view(1, 1, 320, 320))\n",
        "  y_hat_np = y_hat.cpu().detach().numpy()\n",
        "  return y_hat_np.reshape((320, 320))\n",
        "\n",
        "def save_img(img, filename):\n",
        "  img_np = img.copy()\n",
        "  img_cv = (img_np * 255.).astype('uint8')\n",
        "  cv2.imwrite(filename, img_cv)"
      ],
      "metadata": {
        "id": "W_tqwuHiY9aq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEST_DIR = r'sample_brains'\n",
        "test_file_names = [name for name in os.listdir(TEST_DIR) if os.path.isfile(os.path.join(TEST_DIR, name))]\n",
        "\n",
        "N_GAIN = 0.06 # Percent intensity of the magnitude of the absolute value of the noise  0.07\n",
        "B_LOW_END = 0.055 # Percent intensity of the darkest part of the B field  0.06\n",
        "\n",
        "B = genCompositeField(SIZE, B_LOW_END)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Resize((320, 320))\n",
        "        ])\n",
        "\n",
        "n4_psnrs = []\n",
        "n4_ssims = []\n",
        "n4_ms_ssims = []\n",
        "n4_losses = []\n",
        "\n",
        "model_psnrs = []\n",
        "model_ssims = []\n",
        "model_ms_ssims = []\n",
        "model_losses = []\n",
        "\n",
        "!mkdir results"
      ],
      "metadata": {
        "id": "MCbEc7x8YVfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run test on model"
      ],
      "metadata": {
        "id": "dX8A1El3Zrtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(len(test_file_names))):\n",
        "\n",
        "  # Get original test image\n",
        "  #img = whole_img[:, :, i].T\n",
        "  name = os.path.join(TEST_DIR, test_file_names[i])\n",
        "  img = cv2.imread(name, cv2.IMREAD_GRAYSCALE)\n",
        "  img = cv2.resize(img, dsize=(320, 320), interpolation=cv2.INTER_CUBIC)\n",
        "  img -= np.min(img)\n",
        "  img = img / np.max(img)\n",
        "\n",
        "  # Make noisy version\n",
        "  mask_np = np.where(img > 0, 1, 0).astype('float32')\n",
        "  img_x = add_rician_noise(normalize(img * B * mask_np), intensity=N_GAIN)\n",
        "\n",
        "  # Get n4 correction\n",
        "  img_n4 = run_n4(img_x, mask_np)\n",
        "\n",
        "  # Get model prediction\n",
        "  with torch.no_grad():\n",
        "    img_y_hat = predict(img_x)\n",
        "  \n",
        "  # Save images\n",
        "  if i % 20 == 0:\n",
        "    save_img(img, \"results/y_\" + str(i) + \".png\")\n",
        "    save_img(img_x, \"results/x_\" + str(i) + \".png\")\n",
        "    save_img(img_n4, \"results/n4_\" + str(i) + \".png\")\n",
        "    save_img(img_y_hat, \"results/y_hat_\" + str(i) + \".png\")\n",
        "\n",
        "  # Convert results to tensors\n",
        "  img_tensor = transform(img).float().view(1, 1, 320, 320)\n",
        "  img_tensor = torch.clamp(img_tensor, 0, 1)\n",
        "  img_n4_tensor = transform(img_n4).float().view(1, 1, 320, 320)\n",
        "  img_n4_tensor = torch.clamp(img_n4_tensor, 0, 1)\n",
        "  img_y_hat_tensor = transform(img_y_hat).float().view(1, 1, 320, 320)\n",
        "  img_y_hat_tensor = torch.clamp(img_y_hat_tensor, 0, 1)\n",
        "\n",
        "  # Evaluate results for n4\n",
        "  psnr = piq.psnr(img_n4_tensor, img_tensor, data_range=1.).item()\n",
        "  ssim = piq.ssim(img_n4_tensor, img_tensor).item()\n",
        "  ms_ssim = piq.multi_scale_ssim(img_n4_tensor, img_tensor, data_range=1., reduction='mean').item()\n",
        "  loss = my_criterion(img_n4_tensor, img_tensor).item()\n",
        "\n",
        "  n4_psnrs.append(psnr)\n",
        "  n4_ssims.append(ssim)\n",
        "  n4_ms_ssims.append(ms_ssim)\n",
        "  n4_losses.append(loss)\n",
        "\n",
        "  # Evaluate results for unet model\n",
        "  psnr = piq.psnr(img_y_hat_tensor, img_tensor, data_range=1.).item()\n",
        "  ssim = piq.ssim(img_y_hat_tensor, img_tensor).item()\n",
        "  ms_ssim = piq.multi_scale_ssim(img_y_hat_tensor, img_tensor, data_range=1., reduction='mean').item()\n",
        "  loss = my_criterion(img_y_hat_tensor, img_tensor).item()\n",
        "\n",
        "  model_psnrs.append(psnr)\n",
        "  model_ssims.append(ssim)\n",
        "  model_ms_ssims.append(ms_ssim)\n",
        "  model_losses.append(loss)"
      ],
      "metadata": {
        "id": "cdzPj2_AZl1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print('PSNR for N4: ' + str(np.array(n4_psnrs).mean()))\n",
        "print('SSIM for N4: ' + str(np.array(n4_ssims).mean()))\n",
        "print('MS-SSIM for N4: ' + str(np.array(n4_ms_ssims).mean()))\n",
        "print('L1+L2 loss for N4: ' + str(np.array(n4_losses).mean()))\n",
        "print('\\n')\n",
        "print('PSNR for model: ' + str(np.array(model_psnrs).mean()))\n",
        "print('SSIM for model: ' + str(np.array(model_ssims).mean()))\n",
        "print('MS-SSIM for model: ' + str(np.array(model_ms_ssims).mean()))\n",
        "print('L1+L2 loss for model: ' + str(np.array(model_losses).mean()))"
      ],
      "metadata": {
        "id": "ux94IaPTaDQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#### 4. Inference the pre-trained model"
      ],
      "metadata": {
        "id": "FKwD9EBkTtnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can download the pre-trained model weights [here](https://drive.google.com/file/d/11MCFPiCKgFMSn52G6Szqt0yGZSV0j7cE/view?usp=sharing).\n",
        "\n",
        "Be sure to upload the .ckpt file to the root directory of this notebook."
      ],
      "metadata": {
        "id": "z6XYs1qIT1By"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_criterion(prediction, y):\n",
        "  return F.mse_loss(prediction, y) + F.l1_loss(prediction, y)\n",
        "\n",
        "# Path to the pre-trained model checkpoint (uploaded by you)\n",
        "# Alternatively, you may use the unet_model trained above\n",
        "ckpt_path = r'/content/epoch=32-step=39600.ckpt'\n",
        "\n",
        "# You need to upload and provide this, we can't provide you with a sample for medical privacy reasons.\n",
        "INPUT_NIFTI_PATH = r'WBD006_Alan_-_PET_MR_WB_Dynami_1_3504.nii'\n",
        "if len(INPUT_NIFTI_PATH) == 0:\n",
        "  print('ERROR: You need to provide the path to your own nifti file for inferencing.')\n",
        "\n",
        "unet_model = LitUNet.load_from_checkpoint(ckpt_path)\n",
        "unet_model.eval()"
      ],
      "metadata": {
        "id": "DeIqf9GeUZvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load nifti\n",
        "nifti_img = nib.load(INPUT_NIFTI_PATH)\n",
        "whole_img = nifti_img.get_fdata()\n",
        "whole_img = whole_img / np.max(whole_img)"
      ],
      "metadata": {
        "id": "f-GdwJzSarhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display middle slice before correction\n",
        "plt.imshow(whole_img[:, :, whole_img.shape[-1] // 2].T, cmap='gray')"
      ],
      "metadata": {
        "id": "knOzTiMbbAvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir predictions\n",
        "\n",
        "# Correct each slice of the input individually\n",
        "for i in tqdm(range(whole_img.shape[-1])):\n",
        "  img = whole_img[:, :, i].T  \n",
        "  img = cv2.resize(img, dsize=(320, 320), interpolation=cv2.INTER_CUBIC)\n",
        "  img -= np.min(img)\n",
        "  img = img / np.max(img)\n",
        "\n",
        "  # Get model prediction\n",
        "  with torch.no_grad():\n",
        "    img_y_hat = predict(img)\n",
        "  \n",
        "  # Save model prediction\n",
        "  save_img(img_y_hat, \"predictions/y_hat_slice_\" + str(i) + \".png\")\n"
      ],
      "metadata": {
        "id": "3j-cctyFazMH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}